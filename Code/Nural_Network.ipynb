{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1958e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Fault Diagnosis Network ===\n",
      "MultiFaultDiagnosisNN(\n",
      "  (batch_norm): BatchNormalizationLayer()\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Fault probabilities: tensor([[0.4689, 0.4588, 0.4534]])\n",
      "Fault predictions: tensor([[0., 0., 0.]])\n",
      "\n",
      "=== Severity Diagnosis Network ===\n",
      "SeverityDiagnosisNN(\n",
      "  (batch_norm): BatchNormalizationLayer()\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "=== Separated Structure ===\n",
      "Number of parameters: 68252\n",
      "\n",
      "=== Joint Structure ===\n",
      "Number of parameters: 17261\n",
      "Joint output shape: torch.Size([1, 9])\n",
      "Decoded fault predictions: tensor([[1., 1., 1.]])\n",
      "Decoded severity predictions: tensor([[0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class BatchNormalizationLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Batch Normalization Layer as described in the paper.\n",
    "    Implements both training and inference modes with moving averages.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, momentum: float = 0.1, eps: float = 1e-5):\n",
    "        super(BatchNormalizationLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.momentum = momentum  # κ in the paper\n",
    "        self.eps = eps  # ϵ in the paper\n",
    "        \n",
    "        # Learnable parameters γ and β\n",
    "        self.gamma = nn.Parameter(torch.ones(input_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
    "        \n",
    "        # Moving averages for inference\n",
    "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('running_var', torch.ones(input_size))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            # Training phase: use batch statistics\n",
    "            batch_mean = x.mean(dim=0)\n",
    "            batch_var = x.var(dim=0, unbiased=False)\n",
    "            \n",
    "            # Update moving averages\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "            \n",
    "            # Normalize using batch statistics\n",
    "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        else:\n",
    "            # Inference phase: use moving averages\n",
    "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
    "        \n",
    "        # Apply scale and shift\n",
    "        return self.gamma * x_normalized + self.beta\n",
    "\n",
    "\n",
    "class MultiFaultDiagnosisNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-fault diagnosis neural network as described in Fig. 2(a) of the paper.\n",
    "    Uses binary cross-entropy loss for multi-label classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, num_faults: int, hidden_layers: List[int] = [128, 64, 32]):\n",
    "        super(MultiFaultDiagnosisNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_faults = num_faults\n",
    "        \n",
    "        # Batch normalization as first layer\n",
    "        self.batch_norm = BatchNormalizationLayer(input_size)\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer with sigmoid activation\n",
    "        layers.append(nn.Linear(prev_size, num_faults))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply batch normalization first\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Pass through FC layers\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        # Apply sigmoid activation for multi-label classification\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def predict(self, x: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
    "        \"\"\"Online inference with threshold decision\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            predictions = (output > threshold).float()\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class SeverityDiagnosisNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Severity diagnosis neural network for individual fault types.\n",
    "    Uses categorical cross-entropy for multi-class classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, num_severity_levels: int = 3, \n",
    "                 hidden_layers: List[int] = [128, 64, 32]):\n",
    "        super(SeverityDiagnosisNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_severity_levels = num_severity_levels\n",
    "        \n",
    "        # Batch normalization as first layer\n",
    "        self.batch_norm = BatchNormalizationLayer(input_size)\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, num_severity_levels))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply batch normalization first\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Pass through FC layers\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        # Apply softmax for multi-class classification\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Online inference with argmax decision\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class SeparatedStructure(nn.Module):\n",
    "    \"\"\"\n",
    "    Separated structure for fault and severity diagnosis.\n",
    "    Contains one fault diagnosis NN and NF severity diagnosis NNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, num_faults: int, num_severity_levels: int = 3,\n",
    "                 hidden_layers: List[int] = [128, 64, 32]):\n",
    "        super(SeparatedStructure, self).__init__()\n",
    "        self.num_faults = num_faults\n",
    "        self.num_severity_levels = num_severity_levels\n",
    "        \n",
    "        # Fault diagnosis network\n",
    "        self.fault_diagnosis = MultiFaultDiagnosisNN(input_size, num_faults, hidden_layers)\n",
    "        \n",
    "        # Severity diagnosis networks (one for each fault type)\n",
    "        self.severity_networks = nn.ModuleList([\n",
    "            SeverityDiagnosisNN(input_size, num_severity_levels, hidden_layers)\n",
    "            for _ in range(num_faults)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        # Diagnose faults\n",
    "        fault_predictions = self.fault_diagnosis(x)\n",
    "        \n",
    "        # Diagnose severity for each fault type\n",
    "        severity_predictions = []\n",
    "        for i, severity_net in enumerate(self.severity_networks):\n",
    "            severity_pred = severity_net(x)\n",
    "            severity_predictions.append(severity_pred)\n",
    "        \n",
    "        return fault_predictions, severity_predictions\n",
    "    \n",
    "    def predict(self, x: torch.Tensor, fault_threshold: float = 0.5) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"Combined prediction for faults and severities\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            fault_pred = self.fault_diagnosis.predict(x, fault_threshold)\n",
    "            severity_preds = []\n",
    "            \n",
    "            for severity_net in self.severity_networks:\n",
    "                severity_pred = severity_net.predict(x)\n",
    "                severity_preds.append(severity_pred)\n",
    "        \n",
    "        return fault_pred, severity_preds\n",
    "\n",
    "\n",
    "class JointStructure(nn.Module):\n",
    "    \"\"\"\n",
    "    Joint structure for simultaneous fault and severity diagnosis.\n",
    "    Uses a single NN for both fault detection and severity classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, num_faults: int, num_severity_levels: int = 3,\n",
    "                 hidden_layers: List[int] = [128, 64, 32]):\n",
    "        super(JointStructure, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_faults = num_faults\n",
    "        self.num_severity_levels = num_severity_levels\n",
    "        self.output_size = num_faults * num_severity_levels\n",
    "        \n",
    "        # Batch normalization as first layer\n",
    "        self.batch_norm = BatchNormalizationLayer(input_size)\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer for joint fault-severity classification\n",
    "        layers.append(nn.Linear(prev_size, self.output_size))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply batch normalization first\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Pass through FC layers\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        # Apply sigmoid activation for multi-label classification\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def predict(self, x: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
    "        \"\"\"Online inference with threshold decision\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            predictions = (output > threshold).float()\n",
    "        return predictions\n",
    "    \n",
    "    def decode_predictions(self, predictions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Decode joint predictions into fault and severity predictions.\n",
    "        Returns: (fault_predictions, severity_predictions)\n",
    "        \"\"\"\n",
    "        batch_size = predictions.shape[0]\n",
    "        fault_predictions = torch.zeros(batch_size, self.num_faults)\n",
    "        severity_predictions = torch.zeros(batch_size, self.num_faults)\n",
    "        \n",
    "        for i in range(self.num_faults):\n",
    "            start_idx = i * self.num_severity_levels\n",
    "            end_idx = start_idx + self.num_severity_levels\n",
    "            \n",
    "            # Check if any severity level is predicted for this fault\n",
    "            fault_severity_preds = predictions[:, start_idx:end_idx]\n",
    "            fault_predictions[:, i] = torch.any(fault_severity_preds > 0.5, dim=1).float()\n",
    "            \n",
    "            # Get the highest severity level predicted\n",
    "            severity_predictions[:, i] = torch.argmax(fault_severity_preds, dim=1).float()\n",
    "        \n",
    "        return fault_predictions, severity_predictions\n",
    "\n",
    "\n",
    "class FaultDiagnosisTrainer:\n",
    "    \"\"\"\n",
    "    Training utilities for fault diagnosis networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def train_multi_fault(self, train_loader, num_epochs: int = 100):\n",
    "        \"\"\"Train multi-fault diagnosis network\"\"\"\n",
    "        self.model.train()\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    def train_severity(self, train_loader, num_epochs: int = 100):\n",
    "        \"\"\"Train severity diagnosis network\"\"\"\n",
    "        self.model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, targets.long())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    def train_joint(self, train_loader, num_epochs: int = 100):\n",
    "        \"\"\"Train joint fault-severity diagnosis network\"\"\"\n",
    "        self.model.train()\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Example parameters\n",
    "    input_size = 50  # Number of KPIs\n",
    "    num_faults = 3   # ERP, ED, EU\n",
    "    num_severity_levels = 3\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Generate sample data\n",
    "    X = torch.randn(1000, input_size)\n",
    "    y_faults = torch.randint(0, 2, (1000, num_faults))  # Multi-label fault data\n",
    "    y_joint = torch.randint(0, 2, (1000, num_faults * num_severity_levels))  # Joint labels\n",
    "    \n",
    "    print(\"=== Multi-Fault Diagnosis Network ===\")\n",
    "    fault_model = MultiFaultDiagnosisNN(input_size, num_faults)\n",
    "    print(fault_model)\n",
    "    \n",
    "    # Test forward pass\n",
    "    with torch.no_grad():\n",
    "        sample_input = torch.randn(1, input_size)\n",
    "        fault_output = fault_model(sample_input)\n",
    "        fault_prediction = fault_model.predict(sample_input)\n",
    "        print(f\"Fault probabilities: {fault_output}\")\n",
    "        print(f\"Fault predictions: {fault_prediction}\")\n",
    "    \n",
    "    print(\"\\n=== Severity Diagnosis Network ===\")\n",
    "    severity_model = SeverityDiagnosisNN(input_size, num_severity_levels)\n",
    "    print(severity_model)\n",
    "    \n",
    "    print(\"\\n=== Separated Structure ===\")\n",
    "    separated_model = SeparatedStructure(input_size, num_faults, num_severity_levels)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in separated_model.parameters())}\")\n",
    "    \n",
    "    print(\"\\n=== Joint Structure ===\")\n",
    "    joint_model = JointStructure(input_size, num_faults, num_severity_levels)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in joint_model.parameters())}\")\n",
    "    \n",
    "    # Test joint structure\n",
    "    with torch.no_grad():\n",
    "        joint_output = joint_model(sample_input)\n",
    "        joint_prediction = joint_model.predict(sample_input)\n",
    "        fault_pred, severity_pred = joint_model.decode_predictions(joint_prediction)\n",
    "        print(f\"Joint output shape: {joint_output.shape}\")\n",
    "        print(f\"Decoded fault predictions: {fault_pred}\")\n",
    "        print(f\"Decoded severity predictions: {severity_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3cfd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

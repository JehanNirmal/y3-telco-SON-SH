{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1958e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Fault Diagnosis Network ===\n",
      "MultiFaultDiagnosisNN(\n",
      "  (batch_norm): BatchNormalizationLayer()\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Fault probabilities: tensor([[0.5310, 0.5158, 0.4802]])\n",
      "Fault predictions: tensor([[1., 1., 0.]])\n",
      "\n",
      "=== Severity Diagnosis Network ===\n",
      "SeverityDiagnosisNN(\n",
      "  (batch_norm): BatchNormalizationLayer()\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "=== Separated Structure ===\n",
      "Number of parameters: 68252\n",
      "\n",
      "=== Joint Structure ===\n",
      "Number of parameters: 17261\n",
      "Joint output shape: torch.Size([1, 9])\n",
      "Decoded fault predictions: tensor([[1., 1., 1.]])\n",
      "Decoded severity predictions: tensor([[0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class BatchNormalizationLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Batch Normalization Layer as described in the paper.\n",
    "    Implements both training and inference modes with moving averages.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, momentum: float = 0.1, eps: float = 1e-5):\n",
    "        super(BatchNormalizationLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.momentum = momentum  # κ in the paper\n",
    "        self.eps = eps  # ϵ in the paper\n",
    "        \n",
    "        # Learnable parameters γ and β\n",
    "        self.gamma = nn.Parameter(torch.ones(input_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
    "        \n",
    "        # Moving averages for inference\n",
    "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('running_var', torch.ones(input_size))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            # Training phase: use batch statistics\n",
    "            batch_mean = x.mean(dim=0)\n",
    "            batch_var = x.var(dim=0, unbiased=False)\n",
    "            \n",
    "            # Update moving averages\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "            \n",
    "            # Normalize using batch statistics\n",
    "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        else:\n",
    "            # Inference phase: use moving averages\n",
    "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
    "        \n",
    "        # Apply scale and shift\n",
    "        return self.gamma * x_normalized + self.beta\n",
    "\n",
    "\n",
    "class MultiFaultDiagnosisNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-fault diagnosis neural network as described in Fig. 2(a) of the paper.\n",
    "    Uses binary cross-entropy loss for multi-label classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, num_faults: int, hidden_layers: List[int] = [128, 64, 32]):\n",
    "        super(MultiFaultDiagnosisNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_faults = num_faults\n",
    "        \n",
    "        # Batch normalization as first layer\n",
    "        self.batch_norm = BatchNormalizationLayer(input_size)\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer with sigmoid activation\n",
    "        layers.append(nn.Linear(prev_size, num_faults))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply batch normalization first\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Pass through FC layers\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        # Apply sigmoid activation for multi-label classification\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def predict(self, x: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
    "        \"\"\"Online inference with threshold decision\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            predictions = (output > threshold).float()\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class SeverityDiagnosisNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Severity diagnosis neural network for individual fault types.\n",
    "    Uses categorical cross-entropy for multi-class classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, num_severity_levels: int = 3, \n",
    "                 hidden_layers: List[int] = [128, 64, 32]):\n",
    "        super(SeverityDiagnosisNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_severity_levels = num_severity_levels\n",
    "        \n",
    "        # Batch normalization as first layer\n",
    "        self.batch_norm = BatchNormalizationLayer(input_size)\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, num_severity_levels))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply batch normalization first\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Pass through FC layers\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        # Apply softmax for multi-class classification\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Online inference with argmax decision\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class SeparatedStructure(nn.Module):\n",
    "    \"\"\"\n",
    "    Separated structure for fault and severity diagnosis.\n",
    "    Contains one fault diagnosis NN and NF severity diagnosis NNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, num_faults: int, num_severity_levels: int = 3,\n",
    "                 hidden_layers: List[int] = [128, 64, 32]):\n",
    "        super(SeparatedStructure, self).__init__()\n",
    "        self.num_faults = num_faults\n",
    "        self.num_severity_levels = num_severity_levels\n",
    "        \n",
    "        # Fault diagnosis network\n",
    "        self.fault_diagnosis = MultiFaultDiagnosisNN(input_size, num_faults, hidden_layers)\n",
    "        \n",
    "        # Severity diagnosis networks (one for each fault type)\n",
    "        self.severity_networks = nn.ModuleList([\n",
    "            SeverityDiagnosisNN(input_size, num_severity_levels, hidden_layers)\n",
    "            for _ in range(num_faults)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        # Diagnose faults\n",
    "        fault_predictions = self.fault_diagnosis(x)\n",
    "        \n",
    "        # Diagnose severity for each fault type\n",
    "        severity_predictions = []\n",
    "        for i, severity_net in enumerate(self.severity_networks):\n",
    "            severity_pred = severity_net(x)\n",
    "            severity_predictions.append(severity_pred)\n",
    "        \n",
    "        return fault_predictions, severity_predictions\n",
    "    \n",
    "    def predict(self, x: torch.Tensor, fault_threshold: float = 0.5) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"Combined prediction for faults and severities\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            fault_pred = self.fault_diagnosis.predict(x, fault_threshold)\n",
    "            severity_preds = []\n",
    "            \n",
    "            for severity_net in self.severity_networks:\n",
    "                severity_pred = severity_net.predict(x)\n",
    "                severity_preds.append(severity_pred)\n",
    "        \n",
    "        return fault_pred, severity_preds\n",
    "\n",
    "\n",
    "class JointStructure(nn.Module):\n",
    "    \"\"\"\n",
    "    Joint structure for simultaneous fault and severity diagnosis.\n",
    "    Uses a single NN for both fault detection and severity classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, num_faults: int, num_severity_levels: int = 3,\n",
    "                 hidden_layers: List[int] = [128, 64, 32]):\n",
    "        super(JointStructure, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_faults = num_faults\n",
    "        self.num_severity_levels = num_severity_levels\n",
    "        self.output_size = num_faults * num_severity_levels\n",
    "        \n",
    "        # Batch normalization as first layer\n",
    "        self.batch_norm = BatchNormalizationLayer(input_size)\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer for joint fault-severity classification\n",
    "        layers.append(nn.Linear(prev_size, self.output_size))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply batch normalization first\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Pass through FC layers\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        # Apply sigmoid activation for multi-label classification\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def predict(self, x: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
    "        \"\"\"Online inference with threshold decision\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            predictions = (output > threshold).float()\n",
    "        return predictions\n",
    "    \n",
    "    def decode_predictions(self, predictions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Decode joint predictions into fault and severity predictions.\n",
    "        Returns: (fault_predictions, severity_predictions)\n",
    "        \"\"\"\n",
    "        batch_size = predictions.shape[0]\n",
    "        fault_predictions = torch.zeros(batch_size, self.num_faults)\n",
    "        severity_predictions = torch.zeros(batch_size, self.num_faults)\n",
    "        \n",
    "        for i in range(self.num_faults):\n",
    "            start_idx = i * self.num_severity_levels\n",
    "            end_idx = start_idx + self.num_severity_levels\n",
    "            \n",
    "            # Check if any severity level is predicted for this fault\n",
    "            fault_severity_preds = predictions[:, start_idx:end_idx]\n",
    "            fault_predictions[:, i] = torch.any(fault_severity_preds > 0.5, dim=1).float()\n",
    "            \n",
    "            # Get the highest severity level predicted\n",
    "            severity_predictions[:, i] = torch.argmax(fault_severity_preds, dim=1).float()\n",
    "        \n",
    "        return fault_predictions, severity_predictions\n",
    "\n",
    "\n",
    "class FaultDiagnosisTrainer:\n",
    "    \"\"\"\n",
    "    Training utilities for fault diagnosis networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def train_multi_fault(self, train_loader, num_epochs: int = 100):\n",
    "        \"\"\"Train multi-fault diagnosis network\"\"\"\n",
    "        self.model.train()\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    def train_severity(self, train_loader, num_epochs: int = 100):\n",
    "        \"\"\"Train severity diagnosis network\"\"\"\n",
    "        self.model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, targets.long())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    def train_joint(self, train_loader, num_epochs: int = 100):\n",
    "        \"\"\"Train joint fault-severity diagnosis network\"\"\"\n",
    "        self.model.train()\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Example parameters\n",
    "    input_size = 50  # Number of KPIs\n",
    "    num_faults = 3   # ERP, ED, EU\n",
    "    num_severity_levels = 3\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Generate sample data\n",
    "    X = torch.randn(1000, input_size)\n",
    "    y_faults = torch.randint(0, 2, (1000, num_faults))  # Multi-label fault data\n",
    "    y_joint = torch.randint(0, 2, (1000, num_faults * num_severity_levels))  # Joint labels\n",
    "    \n",
    "    print(\"=== Multi-Fault Diagnosis Network ===\")\n",
    "    fault_model = MultiFaultDiagnosisNN(input_size, num_faults)\n",
    "    print(fault_model)\n",
    "    \n",
    "    # Test forward pass\n",
    "    with torch.no_grad():\n",
    "        sample_input = torch.randn(1, input_size)\n",
    "        fault_output = fault_model(sample_input)\n",
    "        fault_prediction = fault_model.predict(sample_input)\n",
    "        print(f\"Fault probabilities: {fault_output}\")\n",
    "        print(f\"Fault predictions: {fault_prediction}\")\n",
    "    \n",
    "    print(\"\\n=== Severity Diagnosis Network ===\")\n",
    "    severity_model = SeverityDiagnosisNN(input_size, num_severity_levels)\n",
    "    print(severity_model)\n",
    "    \n",
    "    print(\"\\n=== Separated Structure ===\")\n",
    "    separated_model = SeparatedStructure(input_size, num_faults, num_severity_levels)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in separated_model.parameters())}\")\n",
    "    \n",
    "    print(\"\\n=== Joint Structure ===\")\n",
    "    joint_model = JointStructure(input_size, num_faults, num_severity_levels)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in joint_model.parameters())}\")\n",
    "    \n",
    "    # Test joint structure\n",
    "    with torch.no_grad():\n",
    "        joint_output = joint_model(sample_input)\n",
    "        joint_prediction = joint_model.predict(sample_input)\n",
    "        fault_pred, severity_pred = joint_model.decode_predictions(joint_prediction)\n",
    "        print(f\"Joint output shape: {joint_output.shape}\")\n",
    "        print(f\"Decoded fault predictions: {fault_pred}\")\n",
    "        print(f\"Decoded severity predictions: {severity_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3cfd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CQI percentiles (10th to 90th): [ 5.8  6.6  7.6  8.8 10.  10.8 11.6 12.6 13.8]\n",
      "CQI normalization range: min=5, max=15\n",
      "\n",
      "RSRP percentiles (10th to 90th): [-116.  -112.  -108.  -104.  -100.   -92.   -84.   -72.8  -58.4]\n",
      "RSRP normalization range: min=-120.0, max=-44.0\n",
      "\n",
      "SINR percentiles (10th to 90th): [10. 10. 11. 13. 15. 17. 19. 22. 26.]\n",
      "SINR normalization range: min=10.0, max=30.0\n",
      "\n",
      "TP percentiles (10th to 90th): [ 5.4  5.8  6.3  6.9  7.5  8.5  9.5 10.4 11.2]\n",
      "TP normalization range: min=5.0, max=12.0\n",
      "\n",
      "Final normalized vector shape: (36,)\n",
      "\n",
      "Output vector: [0.38666667 0.44       0.50666667 0.58666667 0.66666667 0.72\n",
      " 0.77333333 0.84       0.92       0.28       0.32       0.36\n",
      " 0.4        0.44       0.52       0.6        0.712      0.856\n",
      " 0.         0.         0.05       0.15       0.25       0.35\n",
      " 0.45       0.6        0.8        0.05714286 0.11428571 0.18571429\n",
      " 0.27142857 0.35714286 0.5        0.64285714 0.77142857 0.88571429]\n",
      "Output shape: (36,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jehan_140826\\AppData\\Local\\Temp\\1\\ipykernel_19904\\78697821.py:113: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  x = preprocess_kpis(df, debug=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_kpis(df: pd.DataFrame, debug: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess network KPIs for SON fault diagnosis as per 2024 IEEE methodology.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with columns 'CQI', 'RSRP', 'SINR', 'TP'.\n",
    "        debug (bool): If True, prints ECDF percentiles and normalization ranges.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized 36-element vector (shape: (36,))\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Input validation\n",
    "    required_cols = ['CQI', 'RSRP', 'SINR', 'TP']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: '{col}'\")\n",
    "    if df[required_cols].isnull().any().any():\n",
    "        raise ValueError(\"Input DataFrame contains missing values in required columns.\")\n",
    "\n",
    "    # CQI: integer in [1, 15]\n",
    "    if not np.issubdtype(df['CQI'].dtype, np.integer):\n",
    "        if not np.all(df['CQI'] == df['CQI'].astype(int)):\n",
    "            raise ValueError(\"CQI values must be integers in [1, 15].\")\n",
    "        df['CQI'] = df['CQI'].astype(int)\n",
    "    if not ((df['CQI'] >= 1) & (df['CQI'] <= 15)).all():\n",
    "        raise ValueError(\"CQI values must be integers in [1, 15].\")\n",
    "\n",
    "    # RSRP: float in [-144, -44]\n",
    "    if not np.issubdtype(df['RSRP'].dtype, np.floating):\n",
    "        df['RSRP'] = df['RSRP'].astype(float)\n",
    "    if not ((df['RSRP'] >= -144) & (df['RSRP'] <= -44)).all():\n",
    "        raise ValueError(\"RSRP values must be floats in [-144, -44].\")\n",
    "\n",
    "    # SINR: non-negative float\n",
    "    if not np.issubdtype(df['SINR'].dtype, np.floating):\n",
    "        df['SINR'] = df['SINR'].astype(float)\n",
    "    if not (df['SINR'] >= 0).all():\n",
    "        raise ValueError(\"SINR values must be non-negative floats.\")\n",
    "\n",
    "    # TP: non-negative float\n",
    "    if not np.issubdtype(df['TP'].dtype, np.floating):\n",
    "        df['TP'] = df['TP'].astype(float)\n",
    "    if not (df['TP'] >= 0).all():\n",
    "        raise ValueError(\"TP values must be non-negative floats.\")\n",
    "\n",
    "    # 2. ECDF and percentile extraction\n",
    "    percentiles = np.arange(10, 100, 10)  # 10th, 20th, ..., 90th\n",
    "    kpi_vectors = []\n",
    "    normalization_info = {}\n",
    "\n",
    "    for kpi, norm in zip(\n",
    "        ['CQI', 'RSRP', 'SINR', 'TP'],\n",
    "        ['cqi', 'rsrp', 'sinr', 'tp']\n",
    "    ):\n",
    "        values = df[kpi].values\n",
    "        # Compute ECDF percentiles using numpy's percentile (linear interpolation)\n",
    "        kpi_percentiles = np.percentile(values, percentiles, interpolation='linear')\n",
    "        kpi_vectors.append(kpi_percentiles)\n",
    "        normalization_info[norm] = {\n",
    "            'raw_percentiles': kpi_percentiles.copy(),\n",
    "            'min': values.min(),\n",
    "            'max': values.max()\n",
    "        }\n",
    "\n",
    "    # 3. Concatenate into a single vector\n",
    "    x = np.concatenate(kpi_vectors)  # shape (36,)\n",
    "\n",
    "    # 4. Normalization\n",
    "    # CQI: [1, 15] -> [0, 1]\n",
    "    x[0:9] = x[0:9] / 15.0\n",
    "\n",
    "    # RSRP: [-144, -44] -> [0, 1]\n",
    "    x[9:18] = (x[9:18] + 144) / 100.0\n",
    "\n",
    "    # SINR: min-max scaling based on observed range\n",
    "    sinr_min, sinr_max = normalization_info['sinr']['min'], normalization_info['sinr']['max']\n",
    "    if sinr_max > sinr_min:\n",
    "        x[18:27] = (x[18:27] - sinr_min) / (sinr_max - sinr_min)\n",
    "    else:\n",
    "        # All values identical: set to 0.0\n",
    "        x[18:27] = 0.0\n",
    "\n",
    "    # TP: min-max scaling based on observed range\n",
    "    tp_min, tp_max = normalization_info['tp']['min'], normalization_info['tp']['max']\n",
    "    if tp_max > tp_min:\n",
    "        x[27:36] = (x[27:36] - tp_min) / (tp_max - tp_min)\n",
    "    else:\n",
    "        x[27:36] = 0.0\n",
    "\n",
    "    # 5. Debug output\n",
    "    if debug:\n",
    "        for i, kpi in enumerate(['CQI', 'RSRP', 'SINR', 'TP']):\n",
    "            print(f\"\\n{kpi} percentiles (10th to 90th): {normalization_info[kpi.lower()]['raw_percentiles']}\")\n",
    "            print(f\"{kpi} normalization range: min={normalization_info[kpi.lower()]['min']}, max={normalization_info[kpi.lower()]['max']}\")\n",
    "        print(f\"\\nFinal normalized vector shape: {x.shape}\")\n",
    "\n",
    "    return x\n",
    "\n",
    "# --- Sample usage example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example DataFrame with 5 UEs\n",
    "    data = {\n",
    "        'CQI': [5, 10, 12, 7, 15],\n",
    "        'RSRP': [-120.0, -100.0, -80.0, -110.0, -44.0],\n",
    "        'SINR': [10.0, 20.0, 15.0, 10.0, 30.0],\n",
    "        'TP': [5.0, 10.0, 7.5, 6.0, 12.0]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    x = preprocess_kpis(df, debug=True)\n",
    "    print(\"\\nOutput vector:\", x)\n",
    "    print(\"Output shape:\", x.shape)  # Should be (36,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455bc400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss=0.6671, Val Loss=0.6635, EMR=0.3950, Macro-F1=0.0000\n",
      "Epoch  10: Train Loss=0.6282, Val Loss=0.6151, EMR=0.3950, Macro-F1=0.0000\n",
      "Early stopping at epoch 11 (no improvement in 10 epochs).\n",
      "Best model restored (EMR=0.3950).\n",
      "\n",
      "Validation metrics:\n",
      "EMR: 0.395\n",
      "macro_f1: 0.0\n",
      "macro_recall: 0.0\n",
      "macro_precision: 0.0\n",
      "FAR: [np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
      "Model saved to multi_fault_model_20250617_135153.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "class MultiFaultDiagnosisNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward neural network for multi-label fault diagnosis in SONs.\n",
    "    Architecture: BN(36) -> FC(84) -> ReLU -> FC(42) -> ReLU -> FC(21) -> ReLU -> FC(3) -> Sigmoid\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=36, output_dim=3):\n",
    "        super(MultiFaultDiagnosisNet, self).__init__()\n",
    "        # BatchNorm on input\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_dim, 84)\n",
    "        self.fc2 = nn.Linear(84, 42)\n",
    "        self.fc3 = nn.Linear(42, 21)\n",
    "        self.fc4 = nn.Linear(21, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input validation: shape and type\n",
    "        if not torch.is_tensor(x):\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\")\n",
    "        if x.ndim != 2 or x.shape[1] != 36:\n",
    "            raise ValueError(f\"Input tensor must have shape (batch_size, 36), got {x.shape}.\")\n",
    "        if not torch.is_floating_point(x):\n",
    "            raise ValueError(\"Input tensor must be of floating point type.\")\n",
    "        if not torch.isfinite(x).all():\n",
    "            raise ValueError(\"Input contains non-finite values (NaN or Inf).\")\n",
    "        # Forward pass\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x  # BCEWithLogitsLoss expects raw logits\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict binary fault labels for input KPI vectors.\n",
    "        Args:\n",
    "            x (torch.Tensor): shape (batch_size, 36)\n",
    "        Returns:\n",
    "            np.ndarray: shape (batch_size, 3), binary labels (0 or 1)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).int().cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "    def save(self, path=None):\n",
    "        \"\"\"\n",
    "        Save model state dict to file with timestamp.\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            path = f\"multi_fault_model_{timestamp}.pt\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load model state dict from file.\n",
    "        \"\"\"\n",
    "        self.load_state_dict(torch.load(path, map_location='cpu'))\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute EMR, macro-F1, recall, precision, and FAR for each fault.\n",
    "        Args:\n",
    "            y_true (np.ndarray): shape (n_samples, 3)\n",
    "            y_pred (np.ndarray): shape (n_samples, 3)\n",
    "        Returns:\n",
    "            dict: metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        # Exact Match Ratio (EMR)\n",
    "        metrics['EMR'] = np.mean(np.all(y_true == y_pred, axis=1))\n",
    "        # Macro F1, recall, precision\n",
    "        metrics['macro_f1'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        metrics['macro_recall'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        metrics['macro_precision'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        # Per-fault FAR: FP / (FP + TN)\n",
    "        far = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            y_t, y_p = y_true[:, i], y_pred[:, i]\n",
    "            fp = np.sum((y_p == 1) & (y_t == 0))\n",
    "            tn = np.sum((y_p == 0) & (y_t == 0))\n",
    "            far_i = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "            far.append(far_i)\n",
    "        metrics['FAR'] = far\n",
    "        return metrics\n",
    "\n",
    "    def train_model(\n",
    "        self, \n",
    "        X_train, y_train, \n",
    "        X_val, y_val, \n",
    "        epochs=100, \n",
    "        batch_size=200, \n",
    "        patience=10, \n",
    "        verbose=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the model with Adam optimizer, BCEWithLogitsLoss, early stopping, and best model saving.\n",
    "        Args:\n",
    "            X_train, y_train: training data (numpy arrays)\n",
    "            X_val, y_val: validation data (numpy arrays)\n",
    "            epochs: max epochs\n",
    "            batch_size: batch size (default 200)\n",
    "            patience: early stopping patience (default 10)\n",
    "            verbose: print metrics every 10 epochs\n",
    "        \"\"\"\n",
    "        # Input checks\n",
    "        if X_train.shape[0] == 0 or X_val.shape[0] == 0:\n",
    "            raise ValueError(\"Training or validation data is empty.\")\n",
    "        if not np.isfinite(X_train).all() or not np.isfinite(X_val).all():\n",
    "            raise ValueError(\"Training or validation data contains non-finite values.\")\n",
    "        if not np.isfinite(y_train).all() or not np.isfinite(y_val).all():\n",
    "            raise ValueError(\"Training or validation labels contain non-finite values.\")\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        # Datasets and loaders\n",
    "        train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "        val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        best_emr = -1\n",
    "        best_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            train_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.forward(xb)\n",
    "                # Numerical stability: clamp logits if needed\n",
    "                loss = criterion(logits, yb)\n",
    "                if not torch.isfinite(loss):\n",
    "                    raise ValueError(\"Non-finite loss encountered during training.\")\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * xb.size(0)\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "\n",
    "            # Validation\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            all_logits, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    logits = self.forward(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                    val_loss += loss.item() * xb.size(0)\n",
    "                    all_logits.append(logits.cpu())\n",
    "                    all_labels.append(yb.cpu())\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            logits = torch.cat(all_logits, dim=0)\n",
    "            labels = torch.cat(all_labels, dim=0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).int().numpy()\n",
    "            labels = labels.int().numpy()\n",
    "            metrics = self.compute_metrics(labels, preds)\n",
    "            emr = metrics['EMR']\n",
    "\n",
    "            # Early stopping and best model saving\n",
    "            if emr > best_emr:\n",
    "                best_emr = emr\n",
    "                best_state = self.state_dict()\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if verbose and (epoch % 10 == 0 or epoch == 1 or epoch == epochs):\n",
    "                print(f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, EMR={emr:.4f}, Macro-F1={metrics['macro_f1']:.4f}\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch} (no improvement in {patience} epochs).\")\n",
    "                break\n",
    "\n",
    "        # Restore best model\n",
    "        if best_state is not None:\n",
    "            self.load_state_dict(best_state)\n",
    "            if verbose:\n",
    "                print(f\"Best model restored (EMR={best_emr:.4f}).\")\n",
    "        else:\n",
    "            print(\"Warning: No improvement during training.\")\n",
    "\n",
    "# --- Sample training script with synthetic data ---\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # Generate synthetic data: 1000 samples, 36 features, 3 labels\n",
    "    n_samples = 1000\n",
    "    X = np.random.rand(n_samples, 36).astype(np.float32)\n",
    "    # Simulate multi-labels: each fault is present with 30% probability\n",
    "    y = (np.random.rand(n_samples, 3) < 0.3).astype(np.float32)\n",
    "    # Split into train/val\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    train_idx, val_idx = idx[:800], idx[800:]\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "    # Instantiate and train model\n",
    "    model = MultiFaultDiagnosisNet()\n",
    "    model.train_model(X_train, y_train, X_val, y_val, epochs=100, batch_size=200, patience=10, verbose=True)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_pred = model.predict(X_val_tensor)\n",
    "    metrics = model.compute_metrics(y_val, y_pred)\n",
    "    print(\"\\nValidation metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Save and load model\n",
    "    model.save()\n",
    "    # model.load('multi_fault_model_YYYYMMDD_HHMMSS.pt')  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc77ec70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss=1.7997, Val Loss=1.8005, EMR_S=0.0267, Macro-F1_S=0.3090\n",
      "Epoch  10: Train Loss=1.7173, Val Loss=1.7549, EMR_S=0.0333, Macro-F1_S=0.3120\n",
      "Early stopping at epoch 15 (no improvement in 10 epochs).\n",
      "Best model restored (EMR_S=0.0467).\n",
      "\n",
      "Test set metrics:\n",
      "EMR_F: 0.3333\n",
      "macro-F1_F: 0.0\n",
      "macro_recall_F: 0.0\n",
      "macro_precision_F: 0.0\n",
      "FAR_F: [np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
      "EMR_S: 0.02\n",
      "macro-F1_S: 0.3043\n",
      "macro_recall_S: 0.308\n",
      "macro_precision_S: 0.3122\n",
      "FAR_S: [np.float64(0.4194), np.float64(0.2518), np.float64(0.3636)]\n",
      "Model saved to separated_severity_model_20250617_135154.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------- Fault Diagnosis Neural Network -------------------\n",
    "class FaultDiagnosisNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Fault diagnosis neural network:\n",
    "    BN(36) -> FC(64) -> ReLU -> FC(32) -> ReLU -> FC(18) -> ReLU -> FC(3) -> Sigmoid\n",
    "    Multi-label output for ERP, ED, EU.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=36, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 18)\n",
    "        self.fc4 = nn.Linear(18, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input validation\n",
    "        if not torch.is_tensor(x):\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\")\n",
    "        if x.ndim != 2 or x.shape[1] != 36:\n",
    "            raise ValueError(f\"Input tensor must have shape (batch_size, 36), got {x.shape}.\")\n",
    "        if not torch.is_floating_point(x):\n",
    "            raise ValueError(\"Input tensor must be of floating point type.\")\n",
    "        if not torch.isfinite(x).all():\n",
    "            raise ValueError(\"Input contains non-finite values (NaN or Inf).\")\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x  # BCEWithLogitsLoss expects raw logits\n",
    "\n",
    "# ------------------- Severity Diagnosis Neural Network -------------------\n",
    "class SeverityDiagnosisNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Severity diagnosis neural network (for one fault):\n",
    "    BN(36) -> FC(16) -> ReLU -> FC(9) -> Softmax\n",
    "    Output: 3 severity levels (slight, medium, severe)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=36, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 9)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input validation\n",
    "        if not torch.is_tensor(x):\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\")\n",
    "        if x.ndim != 2 or x.shape[1] != 36:\n",
    "            raise ValueError(f\"Input tensor must have shape (batch_size, 36), got {x.shape}.\")\n",
    "        if not torch.is_floating_point(x):\n",
    "            raise ValueError(\"Input tensor must be of floating point type.\")\n",
    "        if not torch.isfinite(x).all():\n",
    "            raise ValueError(\"Input contains non-finite values (NaN or Inf).\")\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x  # CrossEntropyLoss expects raw logits\n",
    "\n",
    "# ------------------- Composite Model -------------------\n",
    "class SeparatedSeverityDiagnosisModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Composite model: combines one fault diagnosis NN and three severity NNs.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fault_nn = FaultDiagnosisNN()\n",
    "        self.severity_nns = nn.ModuleList([SeverityDiagnosisNN() for _ in range(3)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Returns: fault_logits (batch, 3), severity_logits (batch, 9)\n",
    "        fault_logits = self.fault_nn(x)\n",
    "        severity_logits = [net(x) for net in self.severity_nns]  # Each: (batch, 3)\n",
    "        severity_logits = torch.cat(severity_logits, dim=1)      # (batch, 9)\n",
    "        return fault_logits, severity_logits\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts faults and severity levels.\n",
    "        Returns:\n",
    "            faults: (batch, 3) binary (0/1)\n",
    "            severity: (batch, 9) one-hot per fault (3 per fault)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            fault_logits, severity_logits = self.forward(x)\n",
    "            faults = (torch.sigmoid(fault_logits) >= 0.5).int()\n",
    "            # For each fault, select argmax severity (one-hot)\n",
    "            severity_preds = []\n",
    "            for i in range(3):\n",
    "                logits = severity_logits[:, i*3:(i+1)*3]\n",
    "                idx = torch.argmax(logits, dim=1)\n",
    "                one_hot = torch.zeros_like(logits)\n",
    "                one_hot[torch.arange(logits.size(0)), idx] = 1\n",
    "                severity_preds.append(one_hot)\n",
    "            severity = torch.cat(severity_preds, dim=1).int()\n",
    "            # Contradiction check: ED+EU both predicted\n",
    "            contradiction = ((faults[:,1] == 1) & (faults[:,2] == 1)).any()\n",
    "            if contradiction:\n",
    "                warnings.warn(\"ED and EU both predicted for at least one sample (contradiction).\")\n",
    "        return faults.cpu().numpy(), severity.cpu().numpy()\n",
    "\n",
    "    def save(self, path=None):\n",
    "        if path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            path = f\"separated_severity_model_{timestamp}.pt\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path, map_location='cpu'))\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(y_fault_true, y_fault_pred, y_sev_true, y_sev_pred):\n",
    "        \"\"\"\n",
    "        Compute metrics for fault and severity diagnosis.\n",
    "        Returns: dict with 4-decimal precision.\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        # Fault metrics\n",
    "        metrics['EMR_F'] = float(np.mean(np.all(y_fault_true == y_fault_pred, axis=1)))\n",
    "        metrics['macro-F1_F'] = float(f1_score(y_fault_true, y_fault_pred, average='macro', zero_division=0))\n",
    "        metrics['macro_recall_F'] = float(recall_score(y_fault_true, y_fault_pred, average='macro', zero_division=0))\n",
    "        metrics['macro_precision_F'] = float(precision_score(y_fault_true, y_fault_pred, average='macro', zero_division=0))\n",
    "        # FAR per fault\n",
    "        far_f = []\n",
    "        for i in range(3):\n",
    "            y_t, y_p = y_fault_true[:, i], y_fault_pred[:, i]\n",
    "            fp = np.sum((y_p == 1) & (y_t == 0))\n",
    "            tn = np.sum((y_p == 0) & (y_t == 0))\n",
    "            far_i = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "            far_f.append(round(far_i, 4))\n",
    "        metrics['FAR_F'] = far_f\n",
    "\n",
    "        # Severity metrics (flattened per-fault one-hot to class index)\n",
    "        n = y_sev_true.shape[0]\n",
    "        y_true_idx = np.argmax(y_sev_true.reshape(n, 3, 3), axis=2)\n",
    "        y_pred_idx = np.argmax(y_sev_pred.reshape(n, 3, 3), axis=2)\n",
    "        # EMR_S: all 3 severities match\n",
    "        metrics['EMR_S'] = float(np.mean(np.all(y_true_idx == y_pred_idx, axis=1)))\n",
    "        # Macro-F1, recall, precision for all severity levels (SLT, MED, SEV)\n",
    "        f1s, recs, precs, fars = [], [], [], []\n",
    "        for sev in range(3):  # 0:SLT, 1:MED, 2:SEV\n",
    "            y_true_flat = (y_true_idx == sev).astype(int).flatten()\n",
    "            y_pred_flat = (y_pred_idx == sev).astype(int).flatten()\n",
    "            f1s.append(f1_score(y_true_flat, y_pred_flat, zero_division=0))\n",
    "            recs.append(recall_score(y_true_flat, y_pred_flat, zero_division=0))\n",
    "            precs.append(precision_score(y_true_flat, y_pred_flat, zero_division=0))\n",
    "            fp = np.sum((y_pred_flat == 1) & (y_true_flat == 0))\n",
    "            tn = np.sum((y_pred_flat == 0) & (y_true_flat == 0))\n",
    "            far = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "            fars.append(round(far, 4))\n",
    "        metrics['macro-F1_S'] = float(np.mean(f1s))\n",
    "        metrics['macro_recall_S'] = float(np.mean(recs))\n",
    "        metrics['macro_precision_S'] = float(np.mean(precs))\n",
    "        metrics['FAR_S'] = fars\n",
    "        # Round all float metrics to 4 decimals\n",
    "        for k in metrics:\n",
    "            if isinstance(metrics[k], float):\n",
    "                metrics[k] = round(metrics[k], 4)\n",
    "        return metrics\n",
    "\n",
    "    def train_model(\n",
    "        self, \n",
    "        X_train, y_fault_train, y_sev_train, \n",
    "        X_val, y_fault_val, y_sev_val, \n",
    "        epochs=100, batch_size=200, patience=10, verbose=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the composite model with separate optimizers and losses.\n",
    "        Early stopping on combined validation loss.\n",
    "        \"\"\"\n",
    "        # Input checks\n",
    "        if X_train.shape[0] == 0 or X_val.shape[0] == 0:\n",
    "            raise ValueError(\"Training or validation data is empty.\")\n",
    "        if not np.isfinite(X_train).all() or not np.isfinite(X_val).all():\n",
    "            raise ValueError(\"Training or validation data contains non-finite values.\")\n",
    "        if not np.isfinite(y_fault_train).all() or not np.isfinite(y_fault_val).all():\n",
    "            raise ValueError(\"Fault labels contain non-finite values.\")\n",
    "        if not np.isfinite(y_sev_train).all() or not np.isfinite(y_sev_val).all():\n",
    "            raise ValueError(\"Severity labels contain non-finite values.\")\n",
    "        if y_fault_train.shape[1] != 3 or y_sev_train.shape[1] != 9:\n",
    "            raise ValueError(\"Fault labels must have shape (batch, 3), severity labels (batch, 9).\")\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        # Datasets and loaders\n",
    "        train_ds = TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_fault_train, dtype=torch.float32),\n",
    "            torch.tensor(y_sev_train, dtype=torch.float32)\n",
    "        )\n",
    "        val_ds = TensorDataset(\n",
    "            torch.tensor(X_val, dtype=torch.float32),\n",
    "            torch.tensor(y_fault_val, dtype=torch.float32),\n",
    "            torch.tensor(y_sev_val, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        # Optimizers\n",
    "        opt_fault = optim.Adam(self.fault_nn.parameters(), lr=5e-4)\n",
    "        opt_sev = optim.Adam(self.severity_nns.parameters(), lr=2.5e-3)\n",
    "        # Losses\n",
    "        loss_fault = nn.BCEWithLogitsLoss()\n",
    "        loss_sev = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_emr_s = -1\n",
    "        best_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            train_loss = 0.0\n",
    "            for xb, yb_fault, yb_sev in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb_fault = yb_fault.to(device)\n",
    "                yb_sev = yb_sev.to(device)\n",
    "                # Forward\n",
    "                fault_logits, severity_logits = self.forward(xb)\n",
    "                # Fault loss\n",
    "                lf = loss_fault(fault_logits, yb_fault)\n",
    "                # Severity loss: for each fault, CrossEntropyLoss\n",
    "                ls = 0.0\n",
    "                for i in range(3):\n",
    "                    # yb_sev: (batch, 9) -> (batch, 3) one-hot for fault i\n",
    "                    y_true = yb_sev[:, i*3:(i+1)*3]\n",
    "                    # Convert one-hot to class index\n",
    "                    y_idx = torch.argmax(y_true, dim=1)\n",
    "                    ls += loss_sev(severity_logits[:, i*3:(i+1)*3], y_idx)\n",
    "                ls /= 3.0\n",
    "                loss = lf + ls\n",
    "                if not torch.isfinite(loss):\n",
    "                    raise ValueError(\"Non-finite loss encountered during training.\")\n",
    "                # Backprop\n",
    "                opt_fault.zero_grad()\n",
    "                opt_sev.zero_grad()\n",
    "                loss.backward()\n",
    "                opt_fault.step()\n",
    "                opt_sev.step()\n",
    "                train_loss += loss.item() * xb.size(0)\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "\n",
    "            # Validation\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            all_fault_logits, all_sev_logits, all_fault_labels, all_sev_labels = [], [], [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb_fault, yb_sev in val_loader:\n",
    "                    xb = xb.to(device)\n",
    "                    yb_fault = yb_fault.to(device)\n",
    "                    yb_sev = yb_sev.to(device)\n",
    "                    fault_logits, severity_logits = self.forward(xb)\n",
    "                    lf = loss_fault(fault_logits, yb_fault)\n",
    "                    ls = 0.0\n",
    "                    for i in range(3):\n",
    "                        y_true = yb_sev[:, i*3:(i+1)*3]\n",
    "                        y_idx = torch.argmax(y_true, dim=1)\n",
    "                        ls += loss_sev(severity_logits[:, i*3:(i+1)*3], y_idx)\n",
    "                    ls /= 3.0\n",
    "                    loss = lf + ls\n",
    "                    val_loss += loss.item() * xb.size(0)\n",
    "                    all_fault_logits.append(fault_logits.cpu())\n",
    "                    all_sev_logits.append(severity_logits.cpu())\n",
    "                    all_fault_labels.append(yb_fault.cpu())\n",
    "                    all_sev_labels.append(yb_sev.cpu())\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            # Predictions and metrics\n",
    "            fault_logits = torch.cat(all_fault_logits, dim=0)\n",
    "            sev_logits = torch.cat(all_sev_logits, dim=0)\n",
    "            fault_labels = torch.cat(all_fault_labels, dim=0)\n",
    "            sev_labels = torch.cat(all_sev_labels, dim=0)\n",
    "            fault_preds = (torch.sigmoid(fault_logits) >= 0.5).int().numpy()\n",
    "            sev_preds = []\n",
    "            for i in range(3):\n",
    "                logits = sev_logits[:, i*3:(i+1)*3]\n",
    "                idx = torch.argmax(logits, dim=1)\n",
    "                one_hot = torch.zeros_like(logits)\n",
    "                one_hot[torch.arange(logits.size(0)), idx] = 1\n",
    "                sev_preds.append(one_hot)\n",
    "            sev_preds = torch.cat(sev_preds, dim=1).int().numpy()\n",
    "            metrics = self.compute_metrics(\n",
    "                fault_labels.int().numpy(), fault_preds,\n",
    "                sev_labels.int().numpy(), sev_preds\n",
    "            )\n",
    "            emr_s = metrics['EMR_S']\n",
    "\n",
    "            # Early stopping and best model saving\n",
    "            if emr_s > best_emr_s:\n",
    "                best_emr_s = emr_s\n",
    "                best_state = self.state_dict()\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if verbose and (epoch % 10 == 0 or epoch == 1 or epoch == epochs):\n",
    "                print(f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, EMR_S={emr_s:.4f}, Macro-F1_S={metrics['macro-F1_S']:.4f}\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch} (no improvement in {patience} epochs).\")\n",
    "                break\n",
    "\n",
    "        # Restore best model\n",
    "        if best_state is not None:\n",
    "            self.load_state_dict(best_state)\n",
    "            if verbose:\n",
    "                print(f\"Best model restored (EMR_S={best_emr_s:.4f}).\")\n",
    "        else:\n",
    "            print(\"Warning: No improvement during training.\")\n",
    "\n",
    "# ------------------- Sample Training Script -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # Synthetic data: 1000 samples, 36 features, 3 faults, 9 severity (3 per fault, one-hot)\n",
    "    n_samples = 1000\n",
    "    X = np.random.rand(n_samples, 36).astype(np.float32)\n",
    "    # Fault labels: each fault present with 30% probability\n",
    "    y_fault = (np.random.rand(n_samples, 3) < 0.3).astype(np.float32)\n",
    "    # Severity labels: for each fault, one-hot (random class)\n",
    "    y_sev = np.zeros((n_samples, 9), dtype=np.float32)\n",
    "    for i in range(3):\n",
    "        idx = np.random.randint(0, 3, size=n_samples)\n",
    "        y_sev[np.arange(n_samples), i*3 + idx] = 1.0\n",
    "    # Split train/val/test\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    train_idx, val_idx, test_idx = idx[:700], idx[700:850], idx[850:]\n",
    "    X_train, y_fault_train, y_sev_train = X[train_idx], y_fault[train_idx], y_sev[train_idx]\n",
    "    X_val, y_fault_val, y_sev_val = X[val_idx], y_fault[val_idx], y_sev[val_idx]\n",
    "    X_test, y_fault_test, y_sev_test = X[test_idx], y_fault[test_idx], y_sev[test_idx]\n",
    "\n",
    "    # Instantiate and train model\n",
    "    model = SeparatedSeverityDiagnosisModel()\n",
    "    model.train_model(\n",
    "        X_train, y_fault_train, y_sev_train,\n",
    "        X_val, y_fault_val, y_sev_val,\n",
    "        epochs=100, batch_size=200, patience=10, verbose=True\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_fault_pred, y_sev_pred = model.predict(X_test_tensor)\n",
    "    metrics = model.compute_metrics(y_fault_test, y_fault_pred, y_sev_test, y_sev_pred)\n",
    "    print(\"\\nTest set metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Save and load model\n",
    "    model.save()\n",
    "    # model.load('separated_severity_model_YYYYMMDD_HHMMSS.pt')  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ae8dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss=0.9254, Val EMR=0.3300, Macro-F1=0.0000\n",
      "Epoch  10: Train Loss=0.8985, Val EMR=0.3300, Macro-F1=0.0000\n",
      "Early stopping at epoch 11 (no improvement in 10 epochs).\n",
      "Best model restored (EMR=0.3300).\n",
      "\n",
      "Source validation metrics:\n",
      "EMR: 0.33\n",
      "macro-F1: 0.0\n",
      "macro_recall: 0.0\n",
      "macro_precision: 0.0\n",
      "FAR: [np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
      "Model saved to mmd_transfer_model_20250617_135156.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------- Neural Network Definition -------------------\n",
    "class FaultDiagnosisNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward neural network for multi-label fault diagnosis.\n",
    "    Architecture: BN(36) -> FC(84) -> ReLU -> FC(42) -> ReLU -> FC(21) -> ReLU -> FC(3)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=36, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, 84)\n",
    "        self.fc2 = nn.Linear(84, 42)\n",
    "        self.fc3 = nn.Linear(42, 21)\n",
    "        self.fc4 = nn.Linear(21, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        # Input validation\n",
    "        if not torch.is_tensor(x):\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\")\n",
    "        if x.ndim != 2 or x.shape[1] != 36:\n",
    "            raise ValueError(f\"Input tensor must have shape (batch_size, 36), got {x.shape}.\")\n",
    "        if not torch.is_floating_point(x):\n",
    "            raise ValueError(\"Input tensor must be of floating point type.\")\n",
    "        if not torch.isfinite(x).all():\n",
    "            raise ValueError(\"Input contains non-finite values (NaN or Inf).\")\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        features = self.relu(self.fc3(x))  # Features for MMD (size 21)\n",
    "        logits = self.fc4(features)\n",
    "        if return_features:\n",
    "            return logits, features\n",
    "        return logits\n",
    "\n",
    "# ------------------- MMD Loss Function -------------------\n",
    "def gaussian_kernel(x, y, bandwidths):\n",
    "    \"\"\"\n",
    "    Computes a multi-kernel Gaussian kernel matrix between x and y.\n",
    "    Args:\n",
    "        x: (n, d)\n",
    "        y: (m, d)\n",
    "        bandwidths: list of kernel bandwidths\n",
    "    Returns:\n",
    "        (n, m) kernel matrix\n",
    "    \"\"\"\n",
    "    xx = x.unsqueeze(1)  # (n, 1, d)\n",
    "    yy = y.unsqueeze(0)  # (1, m, d)\n",
    "    L2_dist = ((xx - yy) ** 2).sum(2)  # (n, m)\n",
    "    kernels = [torch.exp(-L2_dist / (2 * bw ** 2)) for bw in bandwidths]\n",
    "    return sum(kernels) / len(kernels)\n",
    "\n",
    "def mmd_loss(source, target, bandwidths=[0.1, 1, 10]):\n",
    "    \"\"\"\n",
    "    Computes Maximum Mean Discrepancy (MMD) between source and target features.\n",
    "    Uses multiple Gaussian kernels for robustness.\n",
    "    MMD = sqrt(||E[φ(X_s)] - E[φ(X_t)]||^2)\n",
    "    \"\"\"\n",
    "    n, m = source.size(0), target.size(0)\n",
    "    if n == 0 or m == 0:\n",
    "        return torch.tensor(0.0, device=source.device)\n",
    "    K_ss = gaussian_kernel(source, source, bandwidths)\n",
    "    K_tt = gaussian_kernel(target, target, bandwidths)\n",
    "    K_st = gaussian_kernel(source, target, bandwidths)\n",
    "    mmd = K_ss.mean() + K_tt.mean() - 2 * K_st.mean()\n",
    "    mmd = torch.sqrt(torch.clamp(mmd, min=0.0))  # Ensure non-negative\n",
    "    return mmd\n",
    "\n",
    "# ------------------- Regularization Loss -------------------\n",
    "def l2_regularization(model):\n",
    "    \"\"\"\n",
    "    Computes L2 norm of all model parameters.\n",
    "    \"\"\"\n",
    "    l2 = torch.tensor(0.0, device=next(model.parameters()).device)\n",
    "    for p in model.parameters():\n",
    "        l2 += torch.norm(p, 2)\n",
    "    return l2\n",
    "\n",
    "# ------------------- Evaluation Metrics -------------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes EMR, macro-F1, recall, precision, FAR for multi-label classification.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics['EMR'] = float(np.mean(np.all(y_true == y_pred, axis=1)))\n",
    "    metrics['macro-F1'] = float(f1_score(y_true, y_pred, average='macro', zero_division=0))\n",
    "    metrics['macro_recall'] = float(recall_score(y_true, y_pred, average='macro', zero_division=0))\n",
    "    metrics['macro_precision'] = float(precision_score(y_true, y_pred, average='macro', zero_division=0))\n",
    "    # FAR per fault\n",
    "    far = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        y_t, y_p = y_true[:, i], y_pred[:, i]\n",
    "        fp = np.sum((y_p == 1) & (y_t == 0))\n",
    "        tn = np.sum((y_p == 0) & (y_t == 0))\n",
    "        far_i = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        far.append(round(far_i, 4))\n",
    "    metrics['FAR'] = far\n",
    "    # Round all float metrics to 4 decimals\n",
    "    for k in metrics:\n",
    "        if isinstance(metrics[k], float):\n",
    "            metrics[k] = round(metrics[k], 4)\n",
    "    return metrics\n",
    "\n",
    "# ------------------- Training Loop -------------------\n",
    "def train_transfer_model(\n",
    "    model, \n",
    "    source_loader, target_loader, \n",
    "    val_loader, \n",
    "    epochs=100, \n",
    "    lambda_mmd=1.0, \n",
    "    lambda_reg=0.01, \n",
    "    lr=5e-4, \n",
    "    patience=10, \n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model with unsupervised transfer learning using MMD loss.\n",
    "    Alternates between source and target batches.\n",
    "    Early stopping on source validation EMR.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_emr = -1\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        source_iter = iter(source_loader)\n",
    "        target_iter = iter(target_loader)\n",
    "        n_batches = min(len(source_loader), len(target_loader))\n",
    "        for _ in range(n_batches):\n",
    "            # Get source batch\n",
    "            try:\n",
    "                xb_s, yb_s = next(source_iter)\n",
    "            except StopIteration:\n",
    "                continue\n",
    "            xb_s, yb_s = xb_s.to(device), yb_s.to(device)\n",
    "            # Get target batch\n",
    "            try:\n",
    "                xb_t = next(target_iter)[0]\n",
    "            except StopIteration:\n",
    "                continue\n",
    "            xb_t = xb_t.to(device)\n",
    "            # Forward: source\n",
    "            logits_s, feat_s = model(xb_s, return_features=True)\n",
    "            # Forward: target\n",
    "            _, feat_t = model(xb_t, return_features=True)\n",
    "            # Losses\n",
    "            cls_loss = criterion(logits_s, yb_s)\n",
    "            mmd = mmd_loss(feat_s, feat_t)\n",
    "            reg = l2_regularization(model)\n",
    "            total_loss = cls_loss + lambda_mmd * mmd + lambda_reg * reg\n",
    "            if not torch.isfinite(total_loss):\n",
    "                raise ValueError(\"Non-finite loss encountered during training.\")\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += total_loss.item() * xb_s.size(0)\n",
    "        if n_batches > 0:\n",
    "            train_loss /= (n_batches * source_loader.batch_size)\n",
    "        else:\n",
    "            train_loss = 0.0\n",
    "\n",
    "        # Validation on source domain\n",
    "        model.eval()\n",
    "        all_logits, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(yb.cpu())\n",
    "        if all_logits:\n",
    "            logits = torch.cat(all_logits, dim=0)\n",
    "            labels = torch.cat(all_labels, dim=0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).int().numpy()\n",
    "            labels = labels.int().numpy()\n",
    "            metrics = compute_metrics(labels, preds)\n",
    "            emr = metrics['EMR']\n",
    "            # Contradiction check: ED+EU both predicted\n",
    "            contradiction = ((preds[:,1] == 1) & (preds[:,2] == 1)).any()\n",
    "            if contradiction:\n",
    "                warnings.warn(\"ED and EU both predicted for at least one sample (contradiction).\")\n",
    "        else:\n",
    "            metrics = {}\n",
    "            emr = 0.0\n",
    "\n",
    "        # Early stopping and best model saving\n",
    "        if emr > best_emr:\n",
    "            best_emr = emr\n",
    "            best_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if verbose and (epoch % 10 == 0 or epoch == 1 or epoch == epochs):\n",
    "            print(f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Val EMR={emr:.4f}, Macro-F1={metrics.get('macro-F1', 0):.4f}\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch} (no improvement in {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    # Restore best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        if verbose:\n",
    "            print(f\"Best model restored (EMR={best_emr:.4f}).\")\n",
    "    else:\n",
    "        print(\"Warning: No improvement during training.\")\n",
    "\n",
    "# ------------------- Model Save/Load -------------------\n",
    "def save_model(model, path=None):\n",
    "    if path is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        path = f\"mmd_transfer_model_{timestamp}.pt\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path, map_location='cpu'))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "# ------------------- Sample Script with Synthetic Data -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # Synthetic source data: 1000 samples, 36 features, 3 labels\n",
    "    n_source = 1000\n",
    "    X_source = np.random.rand(n_source, 36).astype(np.float32)\n",
    "    y_source = (np.random.rand(n_source, 3) < 0.3).astype(np.float32)\n",
    "    # Synthetic target data: 1000 samples, 36 features, no labels\n",
    "    n_target = 1000\n",
    "    X_target = np.random.rand(n_target, 36).astype(np.float32)\n",
    "    # Split source into train/val\n",
    "    idx = np.random.permutation(n_source)\n",
    "    train_idx, val_idx = idx[:800], idx[800:]\n",
    "    X_train, y_train = X_source[train_idx], y_source[train_idx]\n",
    "    X_val, y_val = X_source[val_idx], y_source[val_idx]\n",
    "    # Target train only (no val)\n",
    "    X_target_train = X_target[:800]\n",
    "\n",
    "    # DataLoaders\n",
    "    batch_size = 200\n",
    "    source_train_loader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    ), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    source_val_loader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32)\n",
    "    ), batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    target_train_loader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_target_train, dtype=torch.float32),\n",
    "    ), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Instantiate and train model\n",
    "    model = FaultDiagnosisNet()\n",
    "    train_transfer_model(\n",
    "        model, \n",
    "        source_train_loader, \n",
    "        target_train_loader, \n",
    "        source_val_loader, \n",
    "        epochs=100, \n",
    "        lambda_mmd=1.0, \n",
    "        lambda_reg=0.01, \n",
    "        lr=5e-4, \n",
    "        patience=10, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Evaluate on source validation set\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in source_val_loader:\n",
    "            logits = model(xb)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    preds = (torch.sigmoid(logits) >= 0.5).int().numpy()\n",
    "    labels = labels.int().numpy()\n",
    "    metrics = compute_metrics(labels, preds)\n",
    "    print(\"\\nSource validation metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Save model\n",
    "    save_model(model)\n",
    "    # To load: load_model(model, 'mmd_transfer_model_YYYYMMDD_HHMMSS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd08dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss=0.8711, Val EMR=0.3300, Macro-F1=0.0000\n",
      "Epoch  10: Train Loss=0.8248, Val EMR=0.3300, Macro-F1=0.0000\n",
      "Early stopping at epoch 11 (no improvement in 10 epochs).\n",
      "Best model restored (EMR=0.3300).\n",
      "\n",
      "Source validation metrics:\n",
      "EMR: 0.33\n",
      "macro-F1: 0.0\n",
      "macro_recall: 0.0\n",
      "macro_precision: 0.0\n",
      "FAR: [np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
      "Model saved to coral_transfer_model_20250617_135157.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------- Neural Network Definition -------------------\n",
    "class FaultDiagnosisNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward neural network for multi-label fault diagnosis.\n",
    "    Architecture: BN(36) -> FC(84) -> ReLU -> FC(42) -> ReLU -> FC(21) -> ReLU -> FC(3)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=36, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, 84)\n",
    "        self.fc2 = nn.Linear(84, 42)\n",
    "        self.fc3 = nn.Linear(42, 21)\n",
    "        self.fc4 = nn.Linear(21, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        # Input validation\n",
    "        if not torch.is_tensor(x):\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\")\n",
    "        if x.ndim != 2 or x.shape[1] != 36:\n",
    "            raise ValueError(f\"Input tensor must have shape (batch_size, 36), got {x.shape}.\")\n",
    "        if not torch.is_floating_point(x):\n",
    "            raise ValueError(\"Input tensor must be of floating point type.\")\n",
    "        if not torch.isfinite(x).all():\n",
    "            raise ValueError(\"Input contains non-finite values (NaN or Inf).\")\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        features = self.relu(self.fc3(x))  # Features for CORAL (size 21)\n",
    "        logits = self.fc4(features)\n",
    "        if return_features:\n",
    "            return logits, features\n",
    "        return logits\n",
    "\n",
    "# ------------------- CORAL Loss Function -------------------\n",
    "def coral_loss(source, target, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Computes CORAL loss between source and target features.\n",
    "    CORAL = ||Cov(X_s) - Cov(X_t)||_F^2 / (4 * d^2)\n",
    "    Cov(X) = (X^T X - (1/n) * (1^T X)^T (1^T X)) / (n-1)\n",
    "    \"\"\"\n",
    "    d = source.size(1)\n",
    "    n_s = source.size(0)\n",
    "    n_t = target.size(0)\n",
    "    # Center features\n",
    "    source_mean = source.mean(dim=0, keepdim=True)\n",
    "    target_mean = target.mean(dim=0, keepdim=True)\n",
    "    source_c = source - source_mean\n",
    "    target_c = target - target_mean\n",
    "    # Covariance matrices\n",
    "    cov_s = (source_c.t() @ source_c) / (n_s - 1)\n",
    "    cov_t = (target_c.t() @ target_c) / (n_t - 1)\n",
    "    # Add epsilon to diagonal for numerical stability\n",
    "    cov_s += torch.eye(d, device=source.device) * epsilon\n",
    "    cov_t += torch.eye(d, device=target.device) * epsilon\n",
    "    # Frobenius norm squared\n",
    "    loss = torch.sum((cov_s - cov_t) ** 2)\n",
    "    loss = loss / (4 * d * d)\n",
    "    return loss\n",
    "\n",
    "# ------------------- Regularization Loss -------------------\n",
    "def l2_regularization(model):\n",
    "    \"\"\"\n",
    "    Computes L2 norm of all model parameters.\n",
    "    \"\"\"\n",
    "    l2 = torch.tensor(0.0, device=next(model.parameters()).device)\n",
    "    for p in model.parameters():\n",
    "        l2 += torch.norm(p, 2)\n",
    "    return l2\n",
    "\n",
    "# ------------------- Evaluation Metrics -------------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes EMR, macro-F1, recall, precision, FAR for multi-label classification.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics['EMR'] = float(np.mean(np.all(y_true == y_pred, axis=1)))\n",
    "    metrics['macro-F1'] = float(f1_score(y_true, y_pred, average='macro', zero_division=0))\n",
    "    metrics['macro_recall'] = float(recall_score(y_true, y_pred, average='macro', zero_division=0))\n",
    "    metrics['macro_precision'] = float(precision_score(y_true, y_pred, average='macro', zero_division=0))\n",
    "    # FAR per fault\n",
    "    far = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        y_t, y_p = y_true[:, i], y_pred[:, i]\n",
    "        fp = np.sum((y_p == 1) & (y_t == 0))\n",
    "        tn = np.sum((y_p == 0) & (y_t == 0))\n",
    "        far_i = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        far.append(round(far_i, 4))\n",
    "    metrics['FAR'] = far\n",
    "    # Round all float metrics to 4 decimals\n",
    "    for k in metrics:\n",
    "        if isinstance(metrics[k], float):\n",
    "            metrics[k] = round(metrics[k], 4)\n",
    "    return metrics\n",
    "\n",
    "# ------------------- Training Loop -------------------\n",
    "def train_transfer_model(\n",
    "    model, \n",
    "    source_loader, target_loader, \n",
    "    val_loader, \n",
    "    epochs=100, \n",
    "    lambda_coral=1.0, \n",
    "    lambda_reg=0.01, \n",
    "    lr=5e-4, \n",
    "    patience=10, \n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model with unsupervised transfer learning using CORAL loss.\n",
    "    Alternates between source and target batches.\n",
    "    Early stopping on source validation EMR.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_emr = -1\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        source_iter = iter(source_loader)\n",
    "        target_iter = iter(target_loader)\n",
    "        n_batches = min(len(source_loader), len(target_loader))\n",
    "        for _ in range(n_batches):\n",
    "            # Get source batch\n",
    "            try:\n",
    "                xb_s, yb_s = next(source_iter)\n",
    "            except StopIteration:\n",
    "                continue\n",
    "            xb_s, yb_s = xb_s.to(device), yb_s.to(device)\n",
    "            # Get target batch\n",
    "            try:\n",
    "                xb_t = next(target_iter)[0]\n",
    "            except StopIteration:\n",
    "                continue\n",
    "            xb_t = xb_t.to(device)\n",
    "            # Forward: source\n",
    "            logits_s, feat_s = model(xb_s, return_features=True)\n",
    "            # Forward: target\n",
    "            _, feat_t = model(xb_t, return_features=True)\n",
    "            # Losses\n",
    "            cls_loss = criterion(logits_s, yb_s)\n",
    "            coral = coral_loss(feat_s, feat_t)\n",
    "            reg = l2_regularization(model)\n",
    "            total_loss = cls_loss + lambda_coral * coral + lambda_reg * reg\n",
    "            if not torch.isfinite(total_loss):\n",
    "                raise ValueError(\"Non-finite loss encountered during training.\")\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += total_loss.item() * xb_s.size(0)\n",
    "        if n_batches > 0:\n",
    "            train_loss /= (n_batches * source_loader.batch_size)\n",
    "        else:\n",
    "            train_loss = 0.0\n",
    "\n",
    "        # Validation on source domain\n",
    "        model.eval()\n",
    "        all_logits, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(yb.cpu())\n",
    "        if all_logits:\n",
    "            logits = torch.cat(all_logits, dim=0)\n",
    "            labels = torch.cat(all_labels, dim=0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).int().numpy()\n",
    "            labels = labels.int().numpy()\n",
    "            metrics = compute_metrics(labels, preds)\n",
    "            emr = metrics['EMR']\n",
    "            # Contradiction check: ED+EU both predicted\n",
    "            contradiction = ((preds[:,1] == 1) & (preds[:,2] == 1)).any()\n",
    "            if contradiction:\n",
    "                warnings.warn(\"ED and EU both predicted for at least one sample (contradiction).\")\n",
    "        else:\n",
    "            metrics = {}\n",
    "            emr = 0.0\n",
    "\n",
    "        # Early stopping and best model saving\n",
    "        if emr > best_emr:\n",
    "            best_emr = emr\n",
    "            best_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if verbose and (epoch % 10 == 0 or epoch == 1 or epoch == epochs):\n",
    "            print(f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Val EMR={emr:.4f}, Macro-F1={metrics.get('macro-F1', 0):.4f}\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch} (no improvement in {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    # Restore best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        if verbose:\n",
    "            print(f\"Best model restored (EMR={best_emr:.4f}).\")\n",
    "    else:\n",
    "        print(\"Warning: No improvement during training.\")\n",
    "\n",
    "# ------------------- Model Save/Load -------------------\n",
    "def save_model(model, path=None):\n",
    "    if path is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        path = f\"coral_transfer_model_{timestamp}.pt\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path, map_location='cpu'))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "# ------------------- Sample Script with Synthetic Data -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # Synthetic source data: 1000 samples, 36 features, 3 labels\n",
    "    n_source = 1000\n",
    "    X_source = np.random.rand(n_source, 36).astype(np.float32)\n",
    "    y_source = (np.random.rand(n_source, 3) < 0.3).astype(np.float32)\n",
    "    # Synthetic target data: 1000 samples, 36 features, no labels\n",
    "    n_target = 1000\n",
    "    X_target = np.random.rand(n_target, 36).astype(np.float32)\n",
    "    # Split source into train/val\n",
    "    idx = np.random.permutation(n_source)\n",
    "    train_idx, val_idx = idx[:800], idx[800:]\n",
    "    X_train, y_train = X_source[train_idx], y_source[train_idx]\n",
    "    X_val, y_val = X_source[val_idx], y_source[val_idx]\n",
    "    # Target train only (no val)\n",
    "    X_target_train = X_target[:800]\n",
    "\n",
    "    # DataLoaders\n",
    "    batch_size = 200\n",
    "    source_train_loader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    ), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    source_val_loader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32)\n",
    "    ), batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    target_train_loader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_target_train, dtype=torch.float32),\n",
    "    ), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Instantiate and train model\n",
    "    model = FaultDiagnosisNet()\n",
    "    train_transfer_model(\n",
    "        model, \n",
    "        source_train_loader, \n",
    "        target_train_loader, \n",
    "        source_val_loader, \n",
    "        epochs=100, \n",
    "        lambda_coral=1.0, \n",
    "        lambda_reg=0.01, \n",
    "        lr=5e-4, \n",
    "        patience=10, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Evaluate on source validation set\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in source_val_loader:\n",
    "            logits = model(xb)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    preds = (torch.sigmoid(logits) >= 0.5).int().numpy()\n",
    "    labels = labels.int().numpy()\n",
    "    metrics = compute_metrics(labels, preds)\n",
    "    print(\"\\nSource validation metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Save model\n",
    "    save_model(model)\n",
    "    # To load: load_model(model, 'coral_transfer_model_YYYYMMDD_HHMMSS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ea105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "EMR_F: 1.0\n",
      "EMR_S: 0.0\n",
      "macro-F1_F: 1.0\n",
      "macro-F1_S: 0.0556\n",
      "fault_recall: [1.0, 1.0, 1.0]\n",
      "fault_precision: [1.0, 1.0, 1.0]\n",
      "fault_FAR: [np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
      "severity_recall: {'SLT': np.float64(0.0), 'MED': np.float64(0.0), 'SEV': np.float64(0.3333)}\n",
      "severity_precision: {'SLT': np.float64(0.0), 'MED': np.float64(0.0), 'SEV': np.float64(0.1111)}\n",
      "severity_FAR: {'SLT': np.float64(0.25), 'MED': np.float64(0.4333), 'SEV': np.float64(0.4111)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "def evaluate_diagnosis_performance(\n",
    "    y_fault_true, y_fault_pred_prob,\n",
    "    y_sev_true, y_sev_pred_prob\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate multi-fault and severity diagnosis performance as per 2024 IEEE paper.\n",
    "\n",
    "    Parameters:\n",
    "        y_fault_true: np.ndarray, shape (n_samples, 3), binary (0/1)\n",
    "        y_fault_pred_prob: np.ndarray, shape (n_samples, 3), [0,1]\n",
    "        y_sev_true: np.ndarray, shape (n_samples, 9), binary (0/1), at most one 1 per fault\n",
    "        y_sev_pred_prob: np.ndarray, shape (n_samples, 9), [0,1]\n",
    "\n",
    "    Returns:\n",
    "        dict with EMR_F, EMR_S, macro-F1_F, macro-F1_S, per-fault and per-severity-level metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------ Input Validation ------------------\n",
    "    n_samples = y_fault_true.shape[0]\n",
    "    # Check shapes\n",
    "    if y_fault_true.shape != (n_samples, 3):\n",
    "        raise ValueError(\"y_fault_true must have shape (n_samples, 3)\")\n",
    "    if y_fault_pred_prob.shape != (n_samples, 3):\n",
    "        raise ValueError(\"y_fault_pred_prob must have shape (n_samples, 3)\")\n",
    "    if y_sev_true.shape != (n_samples, 9):\n",
    "        raise ValueError(\"y_sev_true must have shape (n_samples, 9)\")\n",
    "    if y_sev_pred_prob.shape != (n_samples, 9):\n",
    "        raise ValueError(\"y_sev_pred_prob must have shape (n_samples, 9)\")\n",
    "    # Check binary true labels\n",
    "    if not np.all(np.isin(y_fault_true, [0, 1])):\n",
    "        raise ValueError(\"y_fault_true must be binary (0 or 1)\")\n",
    "    if not np.all(np.isin(y_sev_true, [0, 1])):\n",
    "        raise ValueError(\"y_sev_true must be binary (0 or 1)\")\n",
    "    # Check probabilities in [0,1]\n",
    "    if not (np.isfinite(y_fault_pred_prob).all() and np.isfinite(y_sev_pred_prob).all()):\n",
    "        raise ValueError(\"Predicted probabilities contain non-finite values\")\n",
    "    if not ((0 <= y_fault_pred_prob).all() and (y_fault_pred_prob <= 1).all()):\n",
    "        raise ValueError(\"y_fault_pred_prob must be in [0, 1]\")\n",
    "    if not ((0 <= y_sev_pred_prob).all() and (y_sev_pred_prob <= 1).all()):\n",
    "        raise ValueError(\"y_sev_pred_prob must be in [0, 1]\")\n",
    "    # Check at most one 1 per fault in severity labels\n",
    "    for i in range(3):\n",
    "        if (y_sev_true[:, i*3:(i+1)*3].sum(axis=1) > 1).any():\n",
    "            raise ValueError(f\"Severity true labels for fault {i} have more than one 1 per sample\")\n",
    "    # Check not empty\n",
    "    if n_samples == 0:\n",
    "        raise ValueError(\"Input arrays are empty\")\n",
    "\n",
    "    # ------------------ Fault Diagnosis Metrics ------------------\n",
    "    # Threshold predicted probabilities at 0.5\n",
    "    y_fault_pred = (y_fault_pred_prob >= 0.5).astype(int)\n",
    "    # EMR_F: all 3 faults correct\n",
    "    EMR_F = float(np.mean(np.all(y_fault_true == y_fault_pred, axis=1)))\n",
    "    # Macro-F1_F: average F1 across 3 faults\n",
    "    macro_F1_F = float(f1_score(y_fault_true, y_fault_pred, average='macro', zero_division=0))\n",
    "    # Recall, precision, FAR per fault\n",
    "    recall_f = []\n",
    "    precision_f = []\n",
    "    far_f = []\n",
    "    for i in range(3):\n",
    "        y_true = y_fault_true[:, i]\n",
    "        y_pred = y_fault_pred[:, i]\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        # FAR = FP / (FP + TN)\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "        far = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        recall_f.append(round(recall, 4))\n",
    "        precision_f.append(round(precision, 4))\n",
    "        far_f.append(round(far, 4))\n",
    "\n",
    "    # ------------------ Severity Diagnosis Metrics ------------------\n",
    "    # For each fault, select argmax of predicted probabilities (one-hot)\n",
    "    y_sev_pred = np.zeros_like(y_sev_pred_prob, dtype=int)\n",
    "    for i in range(3):\n",
    "        idx = np.argmax(y_sev_pred_prob[:, i*3:(i+1)*3], axis=1)\n",
    "        y_sev_pred[np.arange(n_samples), i*3 + idx] = 1\n",
    "    # EMR_S: all 9 severity labels correct (per sample)\n",
    "    EMR_S = float(np.mean(np.all(y_sev_true == y_sev_pred, axis=1)))\n",
    "    # Macro-F1_S: average F1 across 9 classes (flattened)\n",
    "    macro_F1_S = float(f1_score(y_sev_true, y_sev_pred, average='macro', zero_division=0))\n",
    "    # Per-severity-level (SLT, MED, SEV) metrics: average across faults\n",
    "    sev_names = ['SLT', 'MED', 'SEV']\n",
    "    recall_s = {k: [] for k in sev_names}\n",
    "    precision_s = {k: [] for k in sev_names}\n",
    "    far_s = {k: [] for k in sev_names}\n",
    "    for j, name in enumerate(sev_names):\n",
    "        for i in range(3):\n",
    "            y_true = y_sev_true[:, i*3 + j]\n",
    "            y_pred = y_sev_pred[:, i*3 + j]\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "            tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "            far = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "            recall_s[name].append(recall)\n",
    "            precision_s[name].append(precision)\n",
    "            far_s[name].append(far)\n",
    "    # Average across faults, round to 4 decimals\n",
    "    recall_s = {k: round(np.mean(v), 4) for k, v in recall_s.items()}\n",
    "    precision_s = {k: round(np.mean(v), 4) for k, v in precision_s.items()}\n",
    "    far_s = {k: round(np.mean(v), 4) for k, v in far_s.items()}\n",
    "\n",
    "    # ------------------ Output Dictionary ------------------\n",
    "    results = {\n",
    "        'EMR_F': round(EMR_F, 4),\n",
    "        'EMR_S': round(EMR_S, 4),\n",
    "        'macro-F1_F': round(macro_F1_F, 4),\n",
    "        'macro-F1_S': round(macro_F1_S, 4),\n",
    "        'fault_recall': recall_f,\n",
    "        'fault_precision': precision_f,\n",
    "        'fault_FAR': far_f,\n",
    "        'severity_recall': recall_s,\n",
    "        'severity_precision': precision_s,\n",
    "        'severity_FAR': far_s\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# ------------------ Sample Usage Example ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    n = 5\n",
    "    # True fault labels: (n, 3)\n",
    "    y_fault_true = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [1, 0, 1]\n",
    "    ])\n",
    "    # Predicted fault probabilities: (n, 3)\n",
    "    y_fault_pred_prob = np.array([\n",
    "        [0.8, 0.2, 0.1],\n",
    "        [0.3, 0.7, 0.2],\n",
    "        [0.6, 0.6, 0.4],\n",
    "        [0.1, 0.2, 0.9],\n",
    "        [0.7, 0.3, 0.8]\n",
    "    ])\n",
    "    # True severity labels: (n, 9) (3 faults × 3 levels)\n",
    "    y_sev_true = np.zeros((n, 9), dtype=int)\n",
    "    y_sev_true[0, 0] = 1  # ERP slight\n",
    "    y_sev_true[1, 4] = 1  # ED medium\n",
    "    y_sev_true[2, 2] = 1  # ERP severe\n",
    "    y_sev_true[2, 3] = 1  # ED slight\n",
    "    y_sev_true[3, 8] = 1  # EU severe\n",
    "    y_sev_true[4, 1] = 1  # ERP medium\n",
    "    y_sev_true[4, 8] = 1  # EU severe\n",
    "    # Predicted severity probabilities: (n, 9)\n",
    "    y_sev_pred_prob = np.random.rand(n, 9)\n",
    "    # Evaluate\n",
    "    results = evaluate_diagnosis_performance(\n",
    "        y_fault_true, y_fault_pred_prob,\n",
    "        y_sev_true, y_sev_pred_prob\n",
    "    )\n",
    "    print(\"Evaluation Results:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd469a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def generate_synthetic_kpi_data(\n",
    "    scenario: str,\n",
    "    n_samples: int,\n",
    "    fault_condition: list,\n",
    "    severity_levels: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic KPI data for SON fault diagnosis (CQI, RSRP, SINR, TP)\n",
    "    for UMa, UMi, RMa scenarios, with configurable faults and severities.\n",
    "\n",
    "    Parameters:\n",
    "        scenario: 'UMa', 'UMi', or 'RMa'\n",
    "        n_samples: number of samples to generate\n",
    "        fault_condition: list of faults (subset of {'ERP', 'ED', 'EU'})\n",
    "        severity_levels: dict mapping faults to severity ('slight', 'medium', 'severe')\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with columns ['CQI', 'RSRP', 'SINR', 'TP']\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Scenario Configs (Table I) ---\n",
    "    scenario = scenario.upper()\n",
    "    if scenario not in ['UMA', 'UMI', 'RMA']:\n",
    "        raise ValueError(\"Scenario must be one of 'UMa', 'UMi', 'RMa' (case-insensitive).\")\n",
    "    configs = {\n",
    "        'UMA': {'bs_power': 49, 'roi': 500, 'inter_bs': 500, 'freq': 2.0, 'bw': 20, 'ue_speed': 3},\n",
    "        'UMI': {'bs_power': 44, 'roi': 200, 'inter_bs': 200, 'freq': 2.6, 'bw': 20, 'ue_speed': 3},\n",
    "        'RMA': {'bs_power': 49, 'roi': 1732, 'inter_bs': 1732, 'freq': 0.8, 'bw': 20, 'ue_speed': 3}\n",
    "    }\n",
    "    cfg = configs[scenario]\n",
    "\n",
    "    # --- Fault Parameter Effects (Table II) ---\n",
    "    # ERP: power loss (dB)\n",
    "    ERP_loss = {'slight': 10, 'medium': 20, 'severe': 30}\n",
    "    # ED: downtilt (deg)\n",
    "    ED_tilt = {'slight': 10, 'medium': 15, 'severe': 20}\n",
    "    # EU: uptilt (deg)\n",
    "    EU_tilt = {'slight': -2, 'medium': -3, 'severe': -7}\n",
    "\n",
    "    # --- Baseline KPI Distributions (Normal) ---\n",
    "    # CQI: mean 11, std 2, truncated [1, 15]\n",
    "    cqi_mean, cqi_std = 11, 2\n",
    "    # RSRP: mean -95, std 8, truncated [-144, -44]\n",
    "    rsrp_mean, rsrp_std = -95, 8\n",
    "    # SINR: mean 15, std 4, truncated [0, 30]\n",
    "    sinr_mean, sinr_std = 15, 4\n",
    "    # TP: mean 10, std 3, truncated [0, 30]\n",
    "    tp_mean, tp_std = 10, 3\n",
    "\n",
    "    # --- Fault Effects ---\n",
    "    # ERP: reduces CQI, RSRP, SINR, TP\n",
    "    if 'ERP' in fault_condition:\n",
    "        sev = severity_levels.get('ERP', 'slight')\n",
    "        loss = ERP_loss[sev]\n",
    "        cqi_mean -= {10:2, 20:4, 30:6}[loss]\n",
    "        rsrp_mean -= loss\n",
    "        sinr_mean -= loss // 2\n",
    "        tp_mean -= loss // 3\n",
    "    # ED: downtilt reduces RSRP, SINR, TP\n",
    "    if 'ED' in fault_condition:\n",
    "        sev = severity_levels.get('ED', 'slight')\n",
    "        tilt = ED_tilt[sev]\n",
    "        rsrp_mean -= tilt // 2\n",
    "        sinr_mean -= tilt // 3\n",
    "        tp_mean -= tilt // 4\n",
    "    # EU: uptilt increases SINR, TP slightly, but may reduce RSRP\n",
    "    if 'EU' in fault_condition:\n",
    "        sev = severity_levels.get('EU', 'slight')\n",
    "        tilt = EU_tilt[sev]\n",
    "        rsrp_mean += tilt // 2  # negative tilt reduces mean\n",
    "        sinr_mean += abs(tilt) // 2\n",
    "        tp_mean += abs(tilt) // 3\n",
    "\n",
    "    # --- Truncated Normal Sampling Helper ---\n",
    "    def tnorm(mean, std, low, high, size):\n",
    "        a, b = (low - mean) / std, (high - mean) / std\n",
    "        return truncnorm.rvs(a, b, loc=mean, scale=std, size=size)\n",
    "\n",
    "    # --- Generate KPIs ---\n",
    "    CQI = np.round(tnorm(cqi_mean, cqi_std, 1, 15, n_samples)).astype(int)\n",
    "    RSRP = tnorm(rsrp_mean, rsrp_std, -144, -44, n_samples)\n",
    "    SINR = tnorm(sinr_mean, sinr_std, 0, 30, n_samples)\n",
    "    TP = tnorm(tp_mean, tp_std, 0, 30, n_samples)\n",
    "\n",
    "    # --- Clip to valid ranges (edge cases) ---\n",
    "    CQI = np.clip(CQI, 1, 15)\n",
    "    RSRP = np.clip(RSRP, -144, -44)\n",
    "    SINR = np.clip(SINR, 0, 30)\n",
    "    TP = np.clip(TP, 0, 30)\n",
    "\n",
    "    # --- Return DataFrame ---\n",
    "    df = pd.DataFrame({\n",
    "        'CQI': CQI,\n",
    "        'RSRP': RSRP,\n",
    "        'SINR': SINR,\n",
    "        'TP': TP\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# ------------------ Sample Usage Example ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: UMa, 10 samples, ERP (severe) and ED (medium)\n",
    "    scenario = 'UMa'\n",
    "    n_samples = 10\n",
    "    fault_condition = ['ERP', 'ED']\n",
    "    severity_levels = {'ERP': 'severe', 'ED': 'medium'}\n",
    "    df = generate_synthetic_kpi_data(scenario, n_samples, fault_condition, severity_levels)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8906405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CQI</th>\n",
       "      <th>RSRP</th>\n",
       "      <th>SINR</th>\n",
       "      <th>TP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>-120.364146</td>\n",
       "      <td>1.645700</td>\n",
       "      <td>2.150841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-140.312502</td>\n",
       "      <td>0.344330</td>\n",
       "      <td>2.386420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-137.403549</td>\n",
       "      <td>3.142195</td>\n",
       "      <td>0.149366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>-141.854354</td>\n",
       "      <td>0.177371</td>\n",
       "      <td>0.814376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>-134.646287</td>\n",
       "      <td>6.968343</td>\n",
       "      <td>0.237294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CQI        RSRP      SINR        TP\n",
       "0    6 -120.364146  1.645700  2.150841\n",
       "1    4 -140.312502  0.344330  2.386420\n",
       "2    5 -137.403549  3.142195  0.149366\n",
       "3    5 -141.854354  0.177371  0.814376\n",
       "4    3 -134.646287  6.968343  0.237294"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4d853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CQI        RSRP       SINR         TP Scenario  Faults Severities\n",
      "0   10 -109.398789  20.151082  10.429746      UMa  Normal           \n",
      "1   12  -95.690721   8.240535  10.542849      UMa  Normal           \n",
      "2   12  -94.143211  12.680529   8.254042      UMa  Normal           \n",
      "3   13  -99.508140  21.592319  12.212510      UMa  Normal           \n",
      "4   11  -93.162488  19.910258   7.337363      UMa  Normal           \n",
      "Total samples: 38400\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# --- Use the generate_synthetic_kpi_data function from previous cell ---\n",
    "\n",
    "def generate_full_synthetic_dataset(n_per_case=200):\n",
    "    scenarios = ['UMa', 'UMi', 'RMa']\n",
    "    faults = ['ERP', 'ED', 'EU']\n",
    "    severities = ['slight', 'medium', 'severe']\n",
    "    all_data = []\n",
    "\n",
    "    # Normal (no fault) for each scenario\n",
    "    for scenario in scenarios:\n",
    "        df = generate_synthetic_kpi_data(\n",
    "            scenario=scenario,\n",
    "            n_samples=n_per_case,\n",
    "            fault_condition=[],\n",
    "            severity_levels={}\n",
    "        )\n",
    "        df['Scenario'] = scenario\n",
    "        df['Faults'] = 'Normal'\n",
    "        df['Severities'] = ''\n",
    "        all_data.append(df)\n",
    "\n",
    "    # All single and multi-fault combinations\n",
    "    for scenario in scenarios:\n",
    "        for n_faults in range(1, 4):\n",
    "            for fault_combo in itertools.combinations(faults, n_faults):\n",
    "                # For each fault, try all severity combinations\n",
    "                for severity_combo in itertools.product(severities, repeat=n_faults):\n",
    "                    fault_condition = list(fault_combo)\n",
    "                    severity_levels = {f: s for f, s in zip(fault_combo, severity_combo)}\n",
    "                    df = generate_synthetic_kpi_data(\n",
    "                        scenario=scenario,\n",
    "                        n_samples=n_per_case,\n",
    "                        fault_condition=fault_condition,\n",
    "                        severity_levels=severity_levels\n",
    "                    )\n",
    "                    df['Scenario'] = scenario\n",
    "                    df['Faults'] = '+'.join(fault_condition)\n",
    "                    df['Severities'] = '+'.join([f\"{f}:{s}\" for f, s in severity_levels.items()])\n",
    "                    all_data.append(df)\n",
    "\n",
    "    # Concatenate all\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "    return full_df\n",
    "\n",
    "# Example: Generate and save the full dataset\n",
    "full_dataset = generate_full_synthetic_dataset(n_per_case=200)\n",
    "print(full_dataset.head())\n",
    "print(\"Total samples:\", len(full_dataset))\n",
    "# Optionally save to CSV\n",
    "full_dataset.to_csv('synthetic_kpi_full_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
